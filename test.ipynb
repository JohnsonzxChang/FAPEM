{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_label = np.ones((250*6,), dtype=float)\n",
    "ad_label[0:125] = 0\n",
    "for k in range(35):\n",
    "    ad_label[125+k] = k/35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_t0 = np.random.randint(125-50, 160+50, size=(5000,), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_label[slice(ad_t0, ad_t0+50, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Global Attention Mechanism\n",
    "class feature_attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size=5, rate=4):\n",
    "        super(feature_attention, self).__init__()\n",
    "        self.nconv = nn.Conv2d(input_dim, output_dim, kernel_size=(1, 1))\n",
    "        self.channel_attention = nn.Sequential(  \n",
    "            nn.Linear(output_dim, int(output_dim / rate)),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "            nn.Linear(int(output_dim / rate), output_dim)  \n",
    "        )\n",
    "        self.spatial_attention = nn.Sequential(  \n",
    "            nn.Conv2d(output_dim, int(output_dim / rate), kernel_size=(1, kernel_size), padding=(0, (kernel_size - 1) // 2)),  \n",
    "            nn.BatchNorm2d(int(output_dim / rate)),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "            nn.Conv2d(int(output_dim / rate), output_dim, kernel_size=(1, kernel_size), padding=(0, (kernel_size - 1) // 2)),  \n",
    "            nn.BatchNorm2d(output_dim)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 2, 1)  # [B, D, N, T]\n",
    "        x = self.nconv(x)  # 扩展数据的特征维度\n",
    "        b, c, n, t = x.shape  \n",
    "        x_permute = x.permute(0, 2, 3, 1)  # [B, N, T, C]\n",
    "        x_att_permute = self.channel_attention(x_permute) \n",
    "        x_channel_att = x_att_permute.permute(0, 3, 1, 2)  # [B, C, N, T]\n",
    "        x = x * x_channel_att\n",
    "        x_spatial_att = self.spatial_attention(x).sigmoid()\n",
    "        out = x * x_spatial_att\n",
    "        return out.permute(0, 3, 2, 1)\n",
    "\n",
    "class AVWGCN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, cheb_k, embed_dim):\n",
    "        \"\"\"\n",
    "        :param in_dim: 输入维度\n",
    "        :param out_dim: 输出维度\n",
    "        :param cheb_k: 切比雪夫多项式的阶，默认为3\n",
    "        :param embed_dim: 节点的嵌入维度\n",
    "        \"\"\"\n",
    "        super(AVWGCN, self).__init__()\n",
    "        self.cheb_k = cheb_k\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed_dim, cheb_k, in_dim, out_dim))\n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed_dim, out_dim))\n",
    "\n",
    "    def forward(self, x, node_embedding):\n",
    "        \"\"\"\n",
    "        :param x: (B, N, C_in)\n",
    "        :param node_embedding: (N, D), 这里的node_embedding是可学习的\n",
    "        :return: (B, N, C_out)\n",
    "        \"\"\"\n",
    "        node_num = node_embedding.shape[0]\n",
    "        # 自适应的学习节点间的内在隐藏关联获取邻接矩阵\n",
    "        # D^(-1/2)AD^(-1/2)=softmax(ReLU(E * E^T)) - (N, N)\n",
    "        support = F.softmax(F.relu(torch.mm(node_embedding, node_embedding.transpose(0, 1))), dim=1)\n",
    "        # 这里得到的support表示标准化的拉普拉斯矩阵\n",
    "        support_set = [torch.eye(node_num).to(support.device), support]\n",
    "        for k in range(2, self.cheb_k):\n",
    "            # Z(k) = 2 * L * Z(k-1) - Z(k-2)\n",
    "            support_set.append(torch.matmul(2 * support, support_set[-1]) - support_set[-2])\n",
    "        supports = torch.stack(support_set, dim=0) # (K, N, N)\n",
    "        # (N, D) * (D, K, C_in, C_out) -> (N, K, C_in, C_out)\n",
    "        weights = torch.einsum('nd, dkio->nkio', node_embedding, self.weights_pool)\n",
    "        # (N, D) * (D, C_out) -> (N, C_out)\n",
    "        bias = torch.matmul(node_embedding, self.bias_pool)\n",
    "\n",
    "        # 多阶切比雪夫计算：(K, N, N) * (B, N, C_in) -> (B, K, N, C_in)\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, x)  # (B, K, N, C_in) 很好奇为什么不在dim=1相加?\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # (B, N, K, C_in) * (N, K, C_in, C_out)\n",
    "        x_gconv = torch.einsum('bnki,nkio->bno', x_g, weights) + bias  # (B, N, C_out)\n",
    "        return x_gconv\n",
    "\n",
    "class AGCRNCell(nn.Module):\n",
    "    def __init__(self, num_node, in_dim, out_dim, cheb_k, embed_dim):\n",
    "        super(AGCRNCell, self).__init__()\n",
    "        self.num_node = num_node\n",
    "        self.hidden_dim = out_dim\n",
    "        self.gate = AVWGCN(in_dim + out_dim, 2 * out_dim, cheb_k, embed_dim)\n",
    "        self.update = AVWGCN(in_dim + out_dim, out_dim, cheb_k, embed_dim)\n",
    "\n",
    "    def forward(self, x, state, node_embedding):\n",
    "        # x: (B, N, C), state: (B, N, D)\n",
    "        state = state.to(x.device)\n",
    "        input_and_state = torch.cat((x, state), dim=-1)\n",
    "        # 两个门控 forget、update\n",
    "        z_r = torch.sigmoid(self.gate(input_and_state, node_embedding))\n",
    "        z, r = torch.split(z_r, self.hidden_dim, dim=-1)\n",
    "        candidate = torch.cat((x, r*state), dim=-1)\n",
    "        hc = torch.tanh(self.update(candidate, node_embedding))\n",
    "        h = z * state + (1 - z) * hc\n",
    "        return h\n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.num_node, self.hidden_dim)\n",
    "\n",
    "class AVWDCRNN(nn.Module):\n",
    "    def __init__(self, num_node, in_dim, out_dim, cheb_k, embed_dim, num_layers=1):\n",
    "        super(AVWDCRNN, self).__init__()\n",
    "        assert num_layers >= 1, \"At least one DCRNN layer in the Encoder.\"\n",
    "        self.num_node = num_node\n",
    "        self.input_dim = in_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dcrnnn_cells = nn.ModuleList()\n",
    "        self.dcrnnn_cells.append(AGCRNCell(num_node, in_dim, out_dim, cheb_k, embed_dim))\n",
    "        for _ in range(1, num_layers):\n",
    "            self.dcrnnn_cells.append(AGCRNCell(num_node, out_dim, out_dim, cheb_k, embed_dim))\n",
    "\n",
    "    def forward(self, x, init_state, node_embedding):\n",
    "        \"\"\"\n",
    "        :param x: (B, T, N, in_dim)\n",
    "        :param init_state: (num_layers, B, N, hidden_dim)\n",
    "        :param node_embedding: (N, D)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        seq_length = x.shape[1]\n",
    "        current_inputs = x\n",
    "        output_hidden = []\n",
    "        for i in range(self.num_layers):\n",
    "            state = init_state[i]\n",
    "            inner_states = []\n",
    "            for t in range(seq_length):\n",
    "                state = self.dcrnnn_cells[i](current_inputs[:, t, :, :], state, node_embedding)\n",
    "                inner_states.append(state)\n",
    "            output_hidden.append(state)  # 最后一个时间步输出的隐藏状态\n",
    "            current_inputs = torch.stack(inner_states, dim=1) # (B, T, N, hid_dim)\n",
    "\n",
    "        # current_inputs: the outputs of last layer: (B, T, N, hidden_dim)\n",
    "        # output_hidden: the last state for each layer: (num_layers, B, N, hidden_dim)\n",
    "        output_hidden = torch.stack(output_hidden, dim=0)\n",
    "        return current_inputs, output_hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init_states = []  # 初始化隐藏层\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.dcrnnn_cells[i].init_hidden_state(batch_size))\n",
    "        return torch.stack(init_states, dim=0)  # (num_layers, B, N, hidden_dim)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, out_dim, max_len=12):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, out_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, out_dim, 2) *\n",
    "                             - math.log(10000.0) / out_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).unsqueeze(2)  # (1, T, 1, D)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, T, N, D) + (1, T, 1, D)\n",
    "        x = x + Variable(self.pe.to(x.device), requires_grad=False)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # 计算在时间维度上的多头注意力机制\n",
    "        self.positional_encoding = PositionalEncoding(embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        # 要求嵌入层特征维度可以被heads整除\n",
    "        assert embed_size % heads == 0\n",
    "        self.head_dim = embed_size // heads   # every head dimension\n",
    "\n",
    "        self.W_V = nn.Linear(self.embed_size, self.head_dim * heads, bias=False)\n",
    "        self.W_K = nn.Linear(self.embed_size, self.head_dim * heads, bias=False)\n",
    "        self.W_Q = nn.Linear(self.embed_size, self.head_dim * heads, bias=False)\n",
    "        # LayerNorm在特征维度上操作\n",
    "        self.norm1 = nn.LayerNorm(self.embed_size)\n",
    "        self.norm2 = nn.LayerNorm(self.embed_size)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embed_size, self.embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_size, self.embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B, T, N, D]\n",
    "        \"\"\"\n",
    "        batch_size, _, _, d_k = x.shape\n",
    "        x = self.positional_encoding(x).permute(0, 2, 1, 3)   # [B, N, T, D]\n",
    "        # 计算Attention的Q、K、V\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        Q = torch.cat(torch.split(Q, self.head_dim, dim=-1), dim=0)  # [k*B, N, T, d_k]\n",
    "        K = torch.cat(torch.split(K, self.head_dim, dim=-1), dim=0)  # [k*B, N, T, d_k]\n",
    "        V = torch.cat(torch.split(V, self.head_dim, dim=-1), dim=0)\n",
    "        # 考虑上下文的长期依赖信息\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float))\n",
    "        attention = F.softmax(scores, dim=-1)  # [k * B, N, T, T]\n",
    "        context = torch.matmul(attention, V)   # context vector\n",
    "        context = torch.cat(torch.split(context, batch_size, dim=0), dim=-1)\n",
    "        context = context + x    # residual connection\n",
    "        out = self.norm1(context)\n",
    "        out = self.fc(out) + context  # residual connection\n",
    "        out = self.norm2(out)\n",
    "        return out\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, adj, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B, T, N, D)\n",
    "        Wh = torch.matmul(h, self.W)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.permute(0, 1, 3, 2)\n",
    "        e = self.leakyrelu(e)\n",
    "\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(self.adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        out = torch.matmul(attention, Wh)\n",
    "        if self.concat:\n",
    "            return F.elu(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_node, input_dim, hidden_dim, output_dim, embed_dim, cheb_k, horizon, num_layers, heads, timesteps, A, kernel_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.A = A\n",
    "        self.timesteps = timesteps\n",
    "        self.num_node = num_node\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.horizon = horizon\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # node embed\n",
    "        self.node_embedding = nn.Parameter(torch.randn(self.num_node, embed_dim), requires_grad=True)\n",
    "        # encoder\n",
    "        self.feature_attention = feature_attention(input_dim=input_dim, output_dim=hidden_dim, kernel_size=kernel_size)\n",
    "        self.encoder = AVWDCRNN(num_node, hidden_dim, hidden_dim, cheb_k, embed_dim, num_layers)\n",
    "        self.GraphAttentionLayer = GraphAttentionLayer(hidden_dim, hidden_dim, A, dropout=0.5, alpha=0.2, concat=True)\n",
    "        self.MultiHeadAttention = MultiHeadAttention(embed_size=hidden_dim, heads=heads)\n",
    "        # predict\n",
    "        self.nconv = nn.Conv2d(1, self.horizon, kernel_size=(1, 1), bias=True)\n",
    "        self.end_conv = nn.Conv2d(hidden_dim, 1, kernel_size=(1, 1), bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, N, D)\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.feature_attention(x)\n",
    "        init_state = self.encoder.init_hidden(batch_size)\n",
    "        output, _ = self.encoder(x, init_state, self.node_embedding)  # (B, T, N, hidden_dim)\n",
    "        state = output[:, -1:, :, :]\n",
    "        state = self.nconv(state)\n",
    "        SAtt = self.GraphAttentionLayer(state)\n",
    "        TAtt = self.MultiHeadAttention(output).permute(0, 2, 1, 3)\n",
    "        out = SAtt + TAtt\n",
    "        out = self.end_conv(out.permute(0, 3, 2, 1))  # [B, 1, N, T] -> [B, N, T]\n",
    "        out = out.permute(0, 3, 2, 1)   # [B, T, N]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(Model(), )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
